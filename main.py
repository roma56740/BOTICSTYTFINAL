import asyncio
import io
import re
import textwrap
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, urljoin
import html
import os
import json
import time

import httpx
from bs4 import BeautifulSoup

from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Flowable
from reportlab.lib.units import mm

from aiogram import Bot, Dispatcher, types, F
from aiogram.enums import ParseMode
from aiogram.client.default import DefaultBotProperties
from aiogram.filters import CommandStart
from aiogram.types import InlineKeyboardButton, InlineKeyboardMarkup, CallbackQuery

# ================== CONFIG ==================
BOT_TOKEN = "7863780174:AAF75id82mMv3RvmlHBVj9ObNpDD-472w8w"
REQUEST_TIMEOUT = 15  # seconds
GPT_MODEL = "gpt-4o"  # –º–æ–∂–µ—à—å —Å–º–µ–Ω–∏—Ç—å –Ω–∞ 'gpt-4-turbo' –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏
# ============================================

# ====== OpenAI client (–Ω–æ–≤—ã–π SDK) ======
# –ù–µ –ø–∞–¥–∞–µ–º, –µ—Å–ª–∏ –ø–∞–∫–µ—Ç –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–æ–¥—Å–∫–∞–∑–∫–∞ –≤ –æ—à–∏–±–∫–µ.
_openai_client = None
def _get_openai_client():
    global _openai_client
    if _openai_client is not None:
        return _openai_client
    try:
        # –Ω–æ–≤—ã–π SDK
        from openai import OpenAI
        _openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        return _openai_client
    except Exception as e:
        raise RuntimeError("OpenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –Ω–µ—Ç –∫–ª—é—á–∞ API. –£—Å—Ç–∞–Ω–æ–≤–∏ –ø–∞–∫–µ—Ç 'openai' –∏ –∑–∞–¥–∞–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é OPENAI_API_KEY.") from e

# ====== Telegram ======
bot = Bot(token=BOT_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
dp = Dispatcher()

# ===== In-memory storage =====
USER_REPORTS: Dict[int, Dict[str, str]] = {}  # {user_id: {"text": str, "pdf_path": str, "pdf_bytes": bytes}}

def esc(s: str) -> str:
    return html.escape(s or "")

# ================== –£—Ç–∏–ª–∏—Ç—ã ==================
def normalize_url(raw: str) -> Optional[str]:
    raw = (raw or "").strip().strip("`")
    if not raw:
        return None
    if not re.match(r"^https?://", raw, flags=re.I):
        raw = "https://" + raw
    try:
        u = urlparse(raw)
        if not u.netloc:
            return None
        base = f"{u.scheme}://{u.netloc}"
        return base
    except Exception:
        return None

async def fetch_text(client: httpx.AsyncClient, url: str) -> Tuple[Optional[str], int, str]:
    try:
        r = await client.get(url, timeout=REQUEST_TIMEOUT, follow_redirects=True, headers={"User-Agent": "Mozilla/5.0"})
        return (r.text, r.status_code, str(r.url))
    except Exception:
        return (None, 0, url)

async def fetch_ok(client: httpx.AsyncClient, url: str) -> bool:
    try:
        r = await client.get(url, timeout=REQUEST_TIMEOUT, follow_redirects=True, headers={"User-Agent": "Mozilla/5.0"})
        return r.status_code < 400
    except Exception:
        return False

def short(text: str, n: int = 180) -> str:
    t = " ".join((text or "").split())
    return t if len(t) <= n else t[: n - 1] + "‚Ä¶"

@dataclass
class AuditItem:
    name: str
    status: str   # "ok" | "warn" | "fail" | "na"
    note: str
    todo: str

# ================== –õ–û–ì–ò –í –ß–ê–¢ ==================
async def send_step(message: types.Message, text: str):
    try:
        return await message.answer(text)
    except Exception:
        # –≤ –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –º–æ–ª—á–∞ –ø—Ä–æ–≥–ª–∞—Ç—ã–≤–∞–µ–º –ª–æ–≥
        return None

# ================== –ê—É–¥–∏—Ç ==================
async def audit_site(base_url: str, message_for_logs: Optional[types.Message] = None) -> Tuple[str, List[AuditItem], Dict[str, List[AuditItem]], Dict]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      summary_text, main_table_items, sections_dict, raw_bundle
      raw_bundle ‚Äî –≤—Å–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è GPT (robots, sitemap, json-ld —Ç–∏–ø—ã –∏ —Ç.–¥.)
    """
    if message_for_logs:
        await send_step(message_for_logs, "üîé –ü–æ–ª—É—á–∞—é –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É‚Ä¶")

    items: List[AuditItem] = []
    sections: Dict[str, List[AuditItem]] = {}
    raw_bundle: Dict[str, any] = {"base_url": base_url}

    async with httpx.AsyncClient(http2=True) as client:
        # 1) –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        html_text, code, final_home = await fetch_text(client, base_url)
        soup = BeautifulSoup(html_text or "", "lxml") if html_text else None
        raw_bundle["home_status_code"] = code
        raw_bundle["home_final_url"] = final_home
        raw_bundle["home_html_excerpt"] = (html_text or "")[:40000]

        if message_for_logs:
            await send_step(message_for_logs, "ü§ñ –ß–∏—Ç–∞—é robots.txt‚Ä¶")

        # 2) robots.txt
        robots_url = urljoin(base_url + "/", "robots.txt")
        robots_txt, robots_code, _ = await fetch_text(client, robots_url)
        has_robots = bool(robots_txt and robots_code < 400)
        raw_bundle["robots_url"] = robots_url
        raw_bundle["robots_status_code"] = robots_code
        raw_bundle["robots_txt"] = robots_txt or ""

        if message_for_logs:
            await send_step(message_for_logs, "üß≠ –ò—â—É sitemap‚Ä¶")

        # 3) sitemap (–∏–∑ robots –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é /sitemap.xml)
        sitemaps = []
        if robots_txt:
            for line in robots_txt.splitlines():
                if line.lower().strip().startswith("sitemap:"):
                    sitemaps.append(line.split(":", 1)[1].strip())
        default_sm = urljoin(base_url + "/", "sitemap.xml")
        if default_sm not in sitemaps:
            if await fetch_ok(client, default_sm):
                sitemaps.append(default_sm)
        raw_bundle["sitemaps"] = sitemaps

        if message_for_logs:
            await send_step(message_for_logs, "üóÇÔ∏è –ü—Ä–æ–≤–µ—Ä—è—é llms.txt / ai.txt‚Ä¶")

        # 4) llms.txt / ai.txt
        llms_url = urljoin(base_url + "/", "llms.txt")
        ai_url = urljoin(base_url + "/", "ai.txt")
        llms_txt, llms_code, _ = await fetch_text(client, llms_url)
        ai_txt, ai_code, _ = await fetch_text(client, ai_url)
        has_llms = bool(llms_txt and llms_code < 400)
        has_ai = bool(ai_txt and ai_code < 400)
        raw_bundle["llms_url"] = llms_url
        raw_bundle["ai_url"] = ai_url
        raw_bundle["llms_status_code"] = llms_code
        raw_bundle["ai_status_code"] = ai_code
        raw_bundle["llms_txt"] = llms_txt or ""
        raw_bundle["ai_txt"] = ai_txt or ""

        if message_for_logs:
            await send_step(message_for_logs, "üì¶ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é JSON-LD‚Ä¶")

        # 5) schema.org JSON-LD
        schema_types = set()
        jsonld_blocks = []
        if soup:
            for s in soup.select('script[type="application/ld+json"]'):
                try:
                    data = json.loads(s.string or "{}")
                    jsonld_blocks.append(data)
                    def collect_types(node):
                        if isinstance(node, dict):
                            t = node.get("@type")
                            if isinstance(t, str):
                                schema_types.add(t)
                            elif isinstance(t, list):
                                schema_types.update([x for x in t if isinstance(x, str)])
                            for v in node.values():
                                collect_types(v)
                        elif isinstance(node, list):
                            for v in node:
                                collect_types(v)
                    collect_types(data)
                except Exception:
                    continue
        raw_bundle["jsonld_types"] = sorted(list(schema_types))
        raw_bundle["jsonld_count"] = len(jsonld_blocks)

        if message_for_logs:
            await send_step(message_for_logs, "üîó –ò—â—É canonical / viewport / —è–∫–æ—Ä—è‚Ä¶")

        # 6) canonical
        has_canonical = bool(soup and soup.find("link", rel=lambda v: v and "canonical" in v.lower()))

        # 7) viewport (mobile)
        has_viewport = bool(soup and soup.find("meta", attrs={"name": "viewport"}))

        # 8) anchors (—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —è–∫–æ—Ä—è)
        anchors_ok = False
        if soup:
            h_ids = [h.get("id") for h in soup.select("h1,h2,h3,h4,h5,h6") if h.get("id")]
            anchors_ok = len(h_ids) >= 3
        raw_bundle["has_canonical"] = has_canonical
        raw_bundle["has_viewport"] = has_viewport
        raw_bundle["anchors_ok"] = anchors_ok

        if message_for_logs:
            await send_step(message_for_logs, "üè† –û—Ü–µ–Ω–∏–≤–∞—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏‚Ä¶")

        # 10) Internal linking
        internal_links = 0
        if soup:
            domain = urlparse(base_url).netloc
            for a in soup.find_all("a", href=True):
                href = a["href"]
                u = urlparse(urljoin(base_url + "/", href))
                if u.netloc == domain:
                    internal_links += 1
        internal_ok = internal_links >= 20
        raw_bundle["internal_links_count"] = internal_links

        # ======== –ì–ª–∞–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ ========
        llm_agents = ["GPTBot", "CCBot", "ClaudeBot", "PerplexityBot"]
        llm_mentions = []
        if robots_txt:
            for agent in llm_agents:
                if re.search(rf"(?i){re.escape(agent)}", robots_txt):
                    llm_mentions.append(agent)

        # robots
        if has_robots and llm_mentions:
            items.append(AuditItem("KI-Crawling robots.txt", "ok",
                                   f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥–∏—Ä–µ–∫—Ç–∏–≤—ã –¥–ª—è: {', '.join(llm_mentions)}",
                                   "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ Allow/Disallow –¥–ª—è LLM-–±–æ—Ç–æ–≤."))
        elif has_robots and not llm_mentions:
            items.append(AuditItem("KI-Crawling robots.txt", "warn",
                                   "–î–æ—Å—Ç—É–ø –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –±–æ—Ç–æ–≤ –µ—Å—Ç—å, –Ω–æ –Ω–µ—Ç —è–≤–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–∏–≤ –¥–ª—è GPTBot/ClaudeBot/Perplexity.",
                                   "–î–æ–±–∞–≤–∏—Ç—å —è–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è LLM-–±–æ—Ç–æ–≤ –≤ robots.txt."))
        else:
            items.append(AuditItem("KI-Crawling robots.txt", "fail",
                                   "robots.txt –Ω–µ –Ω–∞–π–¥–µ–Ω.",
                                   "–°–æ–∑–¥–∞—Ç—å robots.txt –∏ —É–∫–∞–∑–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –æ–±—Ö–æ–¥–∞."))

        # llms/ai
        if has_llms or has_ai:
            which = "llms.txt" if has_llms else "ai.txt"
            body = (llms_txt or ai_txt or "").strip()
            has_policy = bool(re.search(r"(?i)policy", body))
            has_contact = bool(re.search(r"(?i)contact", body))
            has_sitemap_kw = bool(re.search(r"(?i)sitemap", body))
            miss = []
            if not has_policy: miss.append("Policy")
            if not has_contact: miss.append("Contact")
            if not has_sitemap_kw: miss.append("Sitemap")
            if miss:
                items.append(AuditItem("llms.txt / ai.txt", "warn",
                                       f"–ù–∞–π–¥–µ–Ω {which}, –Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ–ª—è: {', '.join(miss)}.",
                                       "–ó–∞–ø–æ–ª–Ω–∏—Ç—å Policy/Contact/Sitemap –≤ —Ñ–∞–π–ª–µ LLM-policy."))
            else:
                items.append(AuditItem("llms.txt / ai.txt", "ok",
                                       f"–ù–∞–π–¥–µ–Ω {which} —Å –∫–ª—é—á–µ–≤—ã–º–∏ –ø–æ–ª—è–º–∏.",
                                       "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∞–∫—Ç—É–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏."))
        else:
            items.append(AuditItem("llms.txt / ai.txt", "fail",
                                   "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–∏—Ç–∏–∫–∞ LLM-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.",
                                   "–°–æ–∑–¥–∞—Ç—å llms.txt (–∏–ª–∏ ai.txt) —Å Policy, Contact –∏ Sitemap."))

        # Schema.org
        if schema_types:
            core = {t for t in schema_types if t in {"Organization", "VideoObject", "FAQPage", "HowTo", "WebPage", "BreadcrumbList"}}
            if core:
                items.append(AuditItem("Schema.org", "warn" if len(core) < 3 else "ok",
                                       f"–ù–∞–π–¥–µ–Ω—ã —Ç–∏–ø—ã: {', '.join(sorted(core))}",
                                       "–†–∞—Å—à–∏—Ä–∏—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ JSON-LD –∫–ª—é—á–µ–≤—ã–º–∏ —Ç–∏–ø–∞–º–∏ (FAQPage, HowTo, WebPage, BreadcrumbList)."))
            else:
                items.append(AuditItem("Schema.org", "warn",
                                       "JSON-LD –Ω–∞–π–¥–µ–Ω, –Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Ç–∏–ø–æ–≤ –º–∞–ª–æ.",
                                       "–î–æ–±–∞–≤–∏—Ç—å FAQPage/HowTo/BreadcrumbList/WebPage."))
        else:
            items.append(AuditItem("Schema.org", "warn",
                                   "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã.",
                                   "–î–æ–±–∞–≤–∏—Ç—å JSON-LD –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π."))

        # Sitemap
        if sitemaps:
            items.append(AuditItem("Sitemap", "ok",
                                   f"–ù–∞–π–¥–µ–Ω–æ: {', '.join(sitemaps[:3])}" + ("‚Ä¶" if len(sitemaps) > 3 else ""),
                                   "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å sitemap —Ä–∞–∑ –≤ –Ω–µ–¥–µ–ª—é."))
        else:
            items.append(AuditItem("Sitemap", "fail",
                                   "Sitemap –Ω–µ –Ω–∞–π–¥–µ–Ω.",
                                   "–î–æ–±–∞–≤–∏—Ç—å sitemap.xml –∏/–∏–ª–∏ —É–∫–∞–∑–∞—Ç—å –µ–≥–æ –≤ robots.txt."))

        # Canonical
        if has_canonical:
            items.append(AuditItem("Indexability / Canonical", "ok",
                                   "–ù–∞ –≥–ª–∞–≤–Ω–æ–π –Ω–∞–π–¥–µ–Ω <link rel='canonical'>.",
                                   "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∫–∞–Ω–æ–Ω–∏–∫–∞–ª—ã."))
        else:
            items.append(AuditItem("Indexability / Canonical", "warn",
                                   "–ö–∞–Ω–æ–Ω–∏–∫–∞–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ –≥–ª–∞–≤–Ω–æ–π.",
                                   "–î–æ–±–∞–≤–∏—Ç—å rel=canonical –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."))

        # Core Web Vitals (MVP)
        items.append(AuditItem("Core Web Vitals", "warn",
                               "–ë–µ–∑ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ CWV ‚Äî –æ—Ü–µ–Ω–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.",
                               "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å LCP/INP/CLS —á–µ—Ä–µ–∑ PageSpeed Insights –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å."))

        # Mobile
        items.append(AuditItem("Mobile", "ok" if has_viewport else "warn",
                               "–ù–∞–ª–∏—á–∏–µ viewport: " + ("–¥–∞" if has_viewport else "–Ω–µ—Ç"),
                               "–î–æ–±–∞–≤–∏—Ç—å <meta name='viewport'> –∏ —Ç–µ—Å—Ç Mobile-Friendly."))

        # Internal Linking
        items.append(AuditItem("Internal Linking", "ok" if internal_ok else "warn",
                               f"–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π: {internal_links}",
                               "–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã/—Ö–∞–±—ã –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ –±–ª–æ–∫–∏."))

        # FAQ / HowTo / Glossary –ø–æ JSON-LD
        faq = "FAQPage" in schema_types
        howto = "HowTo" in schema_types
        if faq or howto:
            items.append(AuditItem("FAQ / HowTo / Glossary", "ok" if (faq and howto) else "warn",
                                   f"FAQPage: {'–¥–∞' if faq else '–Ω–µ—Ç'}, HowTo: {'–¥–∞' if howto else '–Ω–µ—Ç'}",
                                   "–†–∞—Å—à–∏—Ä–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Q&A/HowTo —Ä–∞–∑–¥–µ–ª—ã."))
        else:
            items.append(AuditItem("FAQ / HowTo / Glossary", "fail",
                                   "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π FAQ/HowTo –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.",
                                   "–î–æ–±–∞–≤–∏—Ç—å FAQPage/HowTo —Å JSON-LD."))

        # EEAT / Brand / Social
        items.append(AuditItem("EEAT", "ok",
                               "–ê–≤—Ç–æ—Ä–∏—Ç–µ—Ç –±—Ä–µ–Ω–¥–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –±–∞–∑–æ–≤–æ (MVP).",
                               "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –∞–≤—Ç–æ—Ä—Å–∫–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏."))
        items.append(AuditItem("Brand / Authority", "ok",
                               "–ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞: –±—Ä–µ–Ω–¥ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç.",
                               "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏ SGE-—Å–Ω–∏–ø–ø–µ—Ç–æ–≤."))
        items.append(AuditItem("Social", "ok" if ( "sameAs" in " ".join(raw_bundle.get("jsonld_types", [])) ) else "warn",
                               "–ù–∞–ª–∏—á–∏–µ sameAs –≤ JSON-LD: " + ("–¥–∞" if ("sameAs" in " ".join(raw_bundle.get("jsonld_types", []))) else "–Ω–µ—Ç"),
                               "–î–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏ sameAs –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏ –≤ JSON-LD."))

        # Freshness / Monitoring
        items.append(AuditItem("Freshness / Monitoring", "warn",
                               "–ù–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–æ—Ç—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤ MVP.",
                               "–î–æ–±–∞–≤–∏—Ç—å RSS/JSON-—Ñ–∏–¥—ã –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–≤–µ–∂–µ—Å—Ç–∏."))

        # GEO Extractability / Anchors
        items.append(AuditItem("GEO Extractability", "warn",
                               "–Ø–≤–Ω—ã–µ –≥–µ–æ-—Å—É—â–Ω–æ—Å—Ç–∏ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤ MVP.",
                               "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç—ã/–∞–¥—Ä–µ—Å–∞ –≤ JSON-LD."))
        items.append(AuditItem("Anchors", "ok" if anchors_ok else "fail",
                               "–°—Ç–∞–±–∏–ª—å–Ω—ã–µ —è–∫–æ—Ä—è –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: " + ("–¥–∞" if anchors_ok else "–Ω–µ—Ç"),
                               "–î–æ–±–∞–≤–∏—Ç—å #anchors (id —É h2/h3) –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."))

        # ======== AI Generative —á–µ–∫-–ª–∏—Å—Ç ========
        ai_sec: List[AuditItem] = []
        ai_sec.append(AuditItem("Q&A", "warn" if not faq else "ok",
                                "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Q&A –æ–≥—Ä–∞–Ω–∏—á–µ–Ω." if not faq else "–ï—Å—Ç—å FAQPage.",
                                "–î–æ–±–∞–≤–∏—Ç—å FAQ —Ä–∞–∑–¥–µ–ª —Å JSON-LD." if not faq else "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å FAQ."))
        ai_sec.append(AuditItem("HowTo", "ok" if howto else "fail",
                                "–ù–∞–ª–∏—á–∏–µ HowTo: " + ("–¥–∞" if howto else "–Ω–µ—Ç"),
                                "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–∞–π–¥—ã –≤ HowTo." if not howto else "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å HowTo."))
        has_lists = bool(soup and len(soup.select("ol,ul")) >= 1)
        ai_sec.append(AuditItem("Answer-Box", "warn",
                                "–ö—Ä–∞—Ç–∫–∏–µ summary –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.",
                                "–î–æ–±–∞–≤–∏—Ç—å –∫—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã/—Ä–µ–∑—é–º–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö."))
        ai_sec.append(AuditItem("Lists / Tables", "ok" if has_lists else "warn",
                                "–°–ø–∏—Å–∫–∏/—Ç–∞–±–ª–∏—Ü—ã –Ω–∞ –≥–ª–∞–≤–Ω–æ–π: " + ("–µ—Å—Ç—å" if has_lists else "–Ω–µ—Ç"),
                                "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–∏—Å–∫–∏ —à–∞–≥–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–π."))
        ai_sec.append(AuditItem("Atomic Answers", "ok" if has_lists else "warn",
                                "–ï—Å—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –∫–∞–∫ –∫—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã." if has_lists else "–ú–∞–ª–æ –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.",
                                "–í—ã–¥–µ–ª–∏—Ç—å –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (–∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ–∞–∫—Ç—ã)."))
        ai_sec.append(AuditItem("Citations", "warn",
                                "–Ø–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.",
                                "–î–æ–±–∞–≤–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏/—Å—Å—ã–ª–∫–∏ –≤ –æ–±—É—á–∞—é—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç."))
        ai_sec.append(AuditItem("JSON-LD", "ok" if schema_types else "warn",
                                f"JSON-LD: {'–µ—Å—Ç—å' if schema_types else '–Ω–µ—Ç'}",
                                "–†–∞—Å—à–∏—Ä–∏—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–∏–ø–∞–º–∏ FAQPage/HowTo/WebPage/BreadcrumbList."))
        ai_sec.append(AuditItem("Licensing", "fail" if not (has_llms or has_ai) else "warn",
                                "–Ø–≤–Ω–æ–π LLM-policy –Ω–µ—Ç" if not (has_llms or has_ai) else "LLM-policy —á–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
                                "–í–∫–ª—é—á–∏—Ç—å –ª–∏—Ü–µ–Ω–∑–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, CC BY 4.0) –≤ LLM-policy."))
        sections["GEO Generative ‚Äî AI-Snippettability"] = ai_sec

        # ======== Prompt ‚Üí Page Map (—É–ø—Ä–æ—â—ë–Ω–Ω–æ –∏–∑ sitemap) ========
        if message_for_logs:
            await send_step(message_for_logs, "üß≠ –°—Ç—Ä–æ—é –∫–∞—Ä—Ç—É –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –∏–∑ sitemap‚Ä¶")

        map_sec: List[AuditItem] = []
        intents = [
            ("–∫–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å", "upload"),
            ("–∫–∞–∫ –º–æ–Ω–µ—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å", "monetiz"),
            ("–∫–∞–∫ —É–¥–∞–ª–∏—Ç—å", "delete"),
            ("—á—Ç–æ —Ç–∞–∫–æ–µ", "about"),
            ("–∫–∞–∫ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å—Å—è", "report"),
        ]
        found_urls = []
        if sitemaps:
            for sm in sitemaps[:2]:
                txt, sc, _ = await fetch_text(client, sm)
                if txt and sc < 400:
                    urls = re.findall(r">([^<]+)</loc>|<loc>([^<]+)</loc>", txt)
                    for a, b in urls:
                        link = a or b
                        if link and link not in found_urls:
                            found_urls.append(link)
                            if len(found_urls) > 200:
                                break

        def pick_url(keyword: str) -> Optional[str]:
            for u in found_urls:
                if keyword in u.lower():
                    return u
            return None

        for intent_text, kw in intents:
            url = pick_url(kw) or base_url
            map_sec.append(AuditItem(intent_text.capitalize(), "ok" if url else "warn",
                                     f"–°—Ç—Ä–∞–Ω–∏—Ü–∞/—Å–µ–∫—Ü–∏—è: {short(url)}",
                                     "–£—Ç–æ—á–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–¥ intent –∏ –¥–æ–±–∞–≤–∏—Ç—å JSON-LD."))
        sections["Prompt ‚Üí Page Map"] = map_sec

        # ======== –û—Ü–µ–Ω–∫–∏ –∏ Top-5 ========
        score = 0
        max_score = 2 * len(items)
        for it in items:
            score += 2 if it.status == "ok" else (1 if it.status == "warn" else 0)
        visibility = round(10 * score / max_score, 1) if max_score else 0.0
        seo_score = round(10 * score / max_score, 1)
        geo_score = round(10 * score / max_score, 1)

        def priority(it: AuditItem) -> Tuple[int, int]:
            p1 = 0 if it.status == "ok" else (1 if it.status == "warn" else 2)
            name_weight = {
                "llms.txt / ai.txt": 3,
                "FAQ / HowTo / Glossary": 2,
                "Core Web Vitals": 2,
                "Anchors": 1,
                "Schema.org": 2,
                "Sitemap": 2,
                "KI-Crawling robots.txt": 2
            }.get(it.name, 0)
            return (p1, name_weight)

        sorted_todos = sorted([i for i in items if i.status != "ok"], key=priority, reverse=True)
        top5 = sorted_todos[:5]

        # ======== –¢–µ–∫—Å—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á—ë—Ç–∞ ========
        def badge(st: str) -> str:
            return {"ok": "‚úÖ", "warn": "üü°", "fail": "‚ùå", "na": "‚ûñ"}.get(st, "‚Ä¢")

        lines: List[str] = []
        domain_disp = urlparse(base_url).netloc
        lines.append(f"<b>–ê—É–¥–∏—Ç —Å–∞–π—Ç–∞: {esc(domain_disp)}</b>")
        if not (has_llms and has_ai):
            lines.append("–ß–∞—Å—Ç—å —Ñ–∞–π–ª–æ–≤ (llms.txt / ai.txt) –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å ‚Äî –æ—Ü–µ–Ω–∫–∞ –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º –¥–∞–Ω–Ω—ã–º.")
        lines.append("\n<b>–ì–ª–∞–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞</b>")
        lines.append("<i>–ö—Ä–∏—Ç–µ—Ä–∏–π ‚Äî –°—Ç–∞—Ç—É—Å ‚Äî –ù–∞–±–ª—é–¥–µ–Ω–∏–µ ‚Äî To-Do</i>")
        for it in items:
            lines.append(
                f"{badge(it.status)} <b>{esc(it.name)}</b>\n"
                f"‚Äî <i>{short(esc(it.note), 220)}</i>\n"
                f"‚Äî To-Do: {short(esc(it.todo), 220)}\n"
            )
        for sec_name, sec_items in sections.items():
            lines.append(f"\n<b>{esc(sec_name)}</b>")
            for it in sec_items:
                lines.append(
                    f"{badge(it.status)} <b>{esc(it.name)}</b> ‚Äî "
                    f"{short(esc(it.note), 200)} ‚Äî "
                    f"To-Do: {short(esc(it.todo), 160)}"
                )
        lines.append("\n<b>–û—Ü–µ–Ω–∫–∏</b>")
        lines.append(f"‚Ä¢ Visibility-Score: {visibility}/10")
        lines.append(f"‚Ä¢ SEO-Score: {seo_score}/10")
        lines.append(f"‚Ä¢ GEO-Score: {geo_score}/10")
        lines.append("\n<b>Top-5 To-Dos (Impact ‚Üí Effort)</b>")
        for idx, it in enumerate(top5, 1):
            lines.append(f"{idx}. {badge(it.status)} <b>{esc(it.name)}</b> ‚Äî {esc(it.todo)}")
        lines.append("\n<b>–†–µ–∑—é–º–µ</b>")
        main_problem = next((i for i in items if i.status == "fail"), None)
        if main_problem:
            lines.append(f"1. –ì–ª–∞–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞: {esc(main_problem.name.lower())} ‚Äî {esc(main_problem.note)}.")
        else:
            lines.append("1. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ MVP-–ø—Ä–æ–≤–µ—Ä–æ–∫.")
        lines.append("2. –ö–ª—é—á–µ–≤–æ–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ 14 –¥–Ω–µ–π: –∑–∞–∫—Ä—ã—Ç—å –ø—É–Ω–∫—Ç—ã –∏–∑ Top-5 To-Dos.")
        summary_text = "\n".join(lines)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–ª–æ–∂–∏–º —á–∏—Å–ª–∞/–∏—Ç–µ–º—ã –≤ raw_bundle –¥–ª—è GPT
        raw_bundle["scores"] = {"visibility": visibility, "seo": seo_score, "geo": geo_score}
        raw_bundle["main_items"] = [it.__dict__ for it in items]
        raw_bundle["sections"] = {k: [it.__dict__ for it in v] for k, v in sections.items()}
        raw_bundle["top5"] = [it.__dict__ for it in top5]

        return summary_text, items, sections, raw_bundle

# ================== GPT-–∞–Ω–∞–ª–∏–∑ ==================
def build_gpt_prompt(base_url: str, raw_bundle: Dict) -> List[Dict]:
    """
    –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è Chat Completions:
    - system: —Ä–æ–ª—å –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å—Ç–∏–ª—é
    - user: –∫—Ä–∞—Ç–∫–æ–µ –¢–ó + —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ (—Å–∂–∞—Ç–æ)
    GPT –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å JSON —Å —Ç—Ä–µ–º—è –ø–æ–ª—è–º–∏:
      - friendly_explainer (–ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, 6‚Äì10 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
      - five_tips (—Å–ø–∏—Å–æ–∫ –∏–∑ 5 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–≤–µ—Ç–æ–≤)
      - final_summary (–∫–æ—Ä–æ—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å –æ—Ü–µ–Ω–∫–∞–º–∏)
    """
    scores = raw_bundle.get("scores", {})
    compact = {
        "base_url": raw_bundle.get("base_url"),
        "scores": scores,
        "robots_present": bool(raw_bundle.get("robots_status_code", 999) < 400),
        "sitemaps": raw_bundle.get("sitemaps", [])[:10],
        "jsonld_types": raw_bundle.get("jsonld_types", [])[:20],
        "has_canonical": raw_bundle.get("has_canonical"),
        "has_viewport": raw_bundle.get("has_viewport"),
        "anchors_ok": raw_bundle.get("anchors_ok"),
        "internal_links_count": raw_bundle.get("internal_links_count"),
        "top5": raw_bundle.get("top5", []),
        "main_items": raw_bundle.get("main_items", [])[:30],
        "sections": {k: v[:30] for k, v in (raw_bundle.get("sections", {})).items()},
    }

    system = (
        "–¢—ã ‚Äî SEO/AI-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –û–±—ä—è—Å–Ω—è–π –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –±–µ–∑ –∂–∞—Ä–≥–æ–Ω–∞. "
        "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ. –í–æ–∑–≤—Ä–∞—â–∞–π —Å—Ç—Ä–æ–≥–æ JSON UTF-8 –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
    )
    user = (
        "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–∞–π—Ç –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å —Ç—Ä–∏ –±–ª–æ–∫–∞:\n"
        "1) friendly_explainer ‚Äî 6‚Äì10 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º: —á—Ç–æ —Å —Å–∞–π—Ç–æ–º –∏ –ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ.\n"
        "2) five_tips ‚Äî 5 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–≤–µ—Ç–æ–≤, –ø–æ–Ω—è—Ç–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫—É –±–µ–∑ —Ç–µ—Ö.—Ñ–æ–Ω–∞. –ö–∞–∂–¥—ã–π ‚Äî –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤.\n"
        "3) final_summary ‚Äî 2‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –æ–±—â–∏–º –≤—ã–≤–æ–¥–æ–º –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –æ—Ü–µ–Ω–æ–∫ (–∏–∑ scores).\n\n"
        "–î–∞–Ω–Ω—ã–µ (—Å–∂–∞—Ç–æ):\n"
        + json.dumps(compact, ensure_ascii=False, indent=2)
    )
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user},
    ]

async def gpt_analyze(base_url: str, raw_bundle: Dict, message_for_logs: Optional[types.Message] = None) -> Dict:
    if message_for_logs:
        await send_step(message_for_logs, "ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞–Ω–Ω—ã–µ –≤ GPT‚Ä¶")

    client = _get_openai_client()
    msgs = build_gpt_prompt(base_url, raw_bundle)
    try:
        resp = client.chat.completions.create(
            model=GPT_MODEL,
            messages=msgs,
            temperature=0.2,
            response_format={"type": "json_object"},
        )
        content = resp.choices[0].message.content
        data = json.loads(content)
        # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        friendly = (data.get("friendly_explainer") or "").strip()
        tips = data.get("five_tips") or []
        if isinstance(tips, str):
            tips = [t.strip() for t in tips.split("\n") if t.strip()]
        tips = tips[:5]
        final_summary = (data.get("final_summary") or "").strip()
        result = {
            "friendly_explainer": friendly,
            "five_tips": tips,
            "final_summary": final_summary
        }
        if message_for_logs:
            await send_step(message_for_logs, "‚úÖ –ê–Ω–∞–ª–∏–∑ –æ—Ç GPT –ø–æ–ª—É—á–µ–Ω.")
        return result
    except Exception as e:
        if message_for_logs:
            await send_step(message_for_logs, f"‚ö†Ô∏è GPT-–∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è: <code>{short(str(e), 200)}</code>\n–ü—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ GPT.")
        # fallback –ø—É—Å—Ç—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        return {"friendly_explainer": "", "five_tips": [], "final_summary": ""}

# ================== PDF –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ==================
def make_pdf_bytes(title: str, table_items: List[AuditItem], sections: Dict[str, List[AuditItem]], gpt_blocks: Dict) -> bytes:
    # —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —à—Ä–∏—Ñ—Ç
    font_path = os.path.join(os.path.dirname(__file__), "fonts", "DejaVuSans.ttf")
    if "DejaVuSans" not in pdfmetrics.getRegisteredFontNames():
        pdfmetrics.registerFont(TTFont("DejaVuSans", font_path))

    buf = io.BytesIO()
    doc = SimpleDocTemplate(
        buf, pagesize=A4,
        leftMargin=14*mm, rightMargin=14*mm,
        topMargin=16*mm, bottomMargin=16*mm
    )
    styles = getSampleStyleSheet()
    styles["Normal"].fontName = "DejaVuSans"
    styles["Title"].fontName = "DejaVuSans"
    styles.add(ParagraphStyle(name="Small", parent=styles["Normal"], fontSize=9, leading=12))
    styles.add(ParagraphStyle(name="H2", parent=styles["Normal"], fontSize=14, leading=18, spaceBefore=8, spaceAfter=4))

    cell = ParagraphStyle("cell", parent=styles["Small"], fontName="DejaVuSans", fontSize=9, leading=12, wordWrap="CJK")
    cell_bold = ParagraphStyle("cell_bold", parent=cell)
    cell_bold.fontName = "DejaVuSans"

    class StatusBadge(Flowable):
        def __init__(self, status: str, width=22*mm, height=6*mm):
            super().__init__()
            self.status = status.lower()
            self.width = width
            self.height = height
        def draw(self):
            c = self.canv
            color_map = {"ok": colors.green, "warn": colors.orange, "fail": colors.red, "na": colors.gray}
            label_map = {"ok": "OK", "warn": "WARN", "fail": "FAIL", "na": "N/A"}
            col = color_map.get(self.status, colors.gray)
            label = label_map.get(self.status, self.status.upper())
            r = 3; x = 2; y = self.height / 2
            c.setFillColor(col); c.circle(x + r, y, r, stroke=0, fill=1)
            c.setFillColor(colors.black); c.setFont("DejaVuSans", 9)
            c.drawString(x + 2*r + 3, y - 3, label)
        def wrap(self, availWidth, availHeight):
            return (self.width, max(self.height, 10))

    def badge_flowable(status: str) -> Flowable:
        return StatusBadge(status)

    flow = []
    flow.append(Paragraph(title, styles["Title"]))
    flow.append(Spacer(1, 6))

    def p(text: str) -> Paragraph:
        safe = html.escape((text or "").strip())
        return Paragraph(safe, cell)

    COL_WIDTHS = [40*mm, 22*mm, 60*mm, 60*mm]

    def items_table(title_txt: str, arr: List[AuditItem]):
        flow.append(Spacer(1, 6))
        flow.append(Paragraph(title_txt, styles["H2"]))
        data = [
            [Paragraph("–ö—Ä–∏—Ç–µ—Ä–∏–π", cell_bold),
             Paragraph("–°—Ç–∞—Ç—É—Å", cell_bold),
             Paragraph("–ù–∞–±–ª—é–¥–µ–Ω–∏–µ", cell_bold),
             Paragraph("To-Do", cell_bold)]
        ]
        for it in arr:
            data.append([p(it.name), badge_flowable(it.status), p(it.note), p(it.todo)])
        t = Table(data, colWidths=COL_WIDTHS, repeatRows=1)
        ts = TableStyle([
            ("FONTNAME", (0,0), (-1,-1), "DejaVuSans"),
            ("FONTSIZE", (0,0), (-1,-1), 9),
            ("LEADING", (0,0), (-1,-1), 12),
            ("BACKGROUND", (0,0), (-1,0), colors.whitesmoke),
            ("TEXTCOLOR", (0,0), (-1,0), colors.black),
            ("ALIGN", (0,0), (-1,-1), "LEFT"),
            ("VALIGN", (0,0), (-1,-1), "TOP"),
            ("LEFTPADDING", (0,0), (-1,-1), 4),
            ("RIGHTPADDING", (0,0), (-1,-1), 4),
            ("TOPPADDING", (0,0), (-1,-1), 4),
            ("BOTTOMPADDING", (0,0), (-1,-1), 4),
            ("GRID", (0,0), (-1,-1), 0.25, colors.lightgrey),
        ])
        t.setStyle(ts)
        t.splitByRow = 1
        flow.append(t)

    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç
    items_table("–ì–ª–∞–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞", table_items)
    for sec_name, sec_items in sections.items():
        items_table(sec_name, sec_items)

    # GPT-–±–ª–æ–∫–∏ –¥–ª—è –ª—é–¥–µ–π
    flow.append(Spacer(1, 10))
    flow.append(Paragraph("–ü–æ–Ω—è—Ç–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (–¥–ª—è –≤–ª–∞–¥–µ–ª—å—Ü–∞ —Å–∞–π—Ç–∞)", styles["H2"]))
    flow.append(Paragraph((gpt_blocks.get("friendly_explainer") or "‚Äî").replace("\n", "<br/>"), styles["Small"]))

    tips = gpt_blocks.get("five_tips") or []
    flow.append(Spacer(1, 6))
    flow.append(Paragraph("5 —Å–æ–≤–µ—Ç–æ–≤ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏", styles["H2"]))
    if tips:
        for i, t in enumerate(tips, 1):
            flow.append(Paragraph(f"{i}. {html.escape(t)}", styles["Small"]))
    else:
        flow.append(Paragraph("‚Äî", styles["Small"]))

    flow.append(Spacer(1, 6))
    flow.append(Paragraph("–û–±—â–∏–π –≤—ã–≤–æ–¥", styles["H2"]))
    flow.append(Paragraph((gpt_blocks.get("final_summary") or "‚Äî").replace("\n", "<br/>"), styles["Small"]))

    doc.build(flow)
    buf.seek(0)
    return buf.read()

# ================== Telegram handlers ==================
@dp.message(CommandStart())
async def on_start(message: types.Message):
    await message.answer(
        "<b>–ü—Ä–∏—à–ª–∏ URL —Å–∞–π—Ç–∞ ‚Äî –∑–∞–ø—É—â—É –∞—É–¥–∏—Ç.</b>\n\n"
        "–Ø —Å–æ–±–µ—Ä—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –æ—Ç–ø—Ä–∞–≤–ª—é –∏—Ö –≤ –ò–ò –¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ –ø—Ä–∏—à–ª—é:\n"
        "1) —Ç–µ—Ö.–æ—Ç—á—ë—Ç, 2) –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, 3) 5 —Å–æ–≤–µ—Ç–æ–≤, 4) PDF.",
        reply_markup=None
    )

@dp.message(F.text)
async def on_url(message: types.Message):
    base = normalize_url(message.text or "")
    if not base:
        await message.answer("‚ö†Ô∏è –û—Ç–ø—Ä–∞–≤—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL, –Ω–∞–ø—Ä–∏–º–µ—Ä: <code>https://example.com</code>")
        return

    await message.answer(f"üîç –ü–æ–Ω—è–ª! –ó–∞–ø—É—Å–∫–∞—é –∞—É–¥–∏—Ç –¥–ª—è: <b>{base}</b>")

    # –ü–æ—à–∞–≥–æ–≤—ã–µ –ª–æ–≥–∏
    log_anchor = await send_step(message, "‚è≥ –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö‚Ä¶")
    try:
        # 1) –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞—É–¥–∏—Ç (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
        tech_text, main_items, sections, raw_bundle = await audit_site(base, message_for_logs=message)
        await send_step(message, "üì¶ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à—ë–Ω.")

        # 2) GPT-–∞–Ω–∞–ª–∏–∑ (–ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ + 5 —Å–æ–≤–µ—Ç–æ–≤ + –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ)
        await send_step(message, "ü§ñ –ì–æ—Ç–æ–≤–ª—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –ø–æ–Ω—è—Ç–Ω–æ–º —è–∑—ã–∫–µ‚Ä¶")
        gpt_blocks = await gpt_analyze(base, raw_bundle, message_for_logs=message)

        # 3) –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        domain_disp = urlparse(base).netloc

        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç: —Ä–µ–∂–µ–º –ø–æ 3500 —Å–∏–º–≤–æ–ª–æ–≤
        await send_step(message, "üßæ –û—Ç–ø—Ä–∞–≤–ª—è—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç‚Ä¶")
        chunks: List[str] = []
        cur = []
        cur_len = 0
        for line in tech_text.split("\n"):
            if cur_len + len(line) + 1 > 3500:
                chunks.append("\n".join(cur))
                cur = []
                cur_len = 0
            cur.append(line)
            cur_len += len(line) + 1
        if cur:
            chunks.append("\n".join(cur))
        for ch in chunks:
            await message.answer(ch)

        # –ü–æ—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        if gpt_blocks.get("friendly_explainer"):
            await send_step(message, "üó£Ô∏è –û—Ç–ø—Ä–∞–≤–ª—è—é –ø–æ–Ω—è—Ç–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ‚Ä¶")
            await message.answer(f"<b>–ü–æ–Ω—è—Ç–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ</b>\n{esc(gpt_blocks['friendly_explainer'])}")

        # 5 —Å–æ–≤–µ—Ç–æ–≤
        tips = gpt_blocks.get("five_tips") or []
        if tips:
            await send_step(message, "üí° –û—Ç–ø—Ä–∞–≤–ª—è—é 5 —Å–æ–≤–µ—Ç–æ–≤‚Ä¶")
            tips_text = "\n".join([f"‚Ä¢ {esc(t)}" for t in tips])
            await message.answer(f"<b>5 —Å–æ–≤–µ—Ç–æ–≤ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é</b>\n{tips_text}")

        # –ö–æ—Ä–æ—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –æ—Ç GPT
        if gpt_blocks.get("final_summary"):
            await message.answer(f"<b>–ò—Ç–æ–≥</b>\n{esc(gpt_blocks['final_summary'])}")

        # 4) PDF —Å–æ –≤—Å–µ–º –≤–º–µ—Å—Ç–µ
        await send_step(message, "üìÑ –§–æ—Ä–º–∏—Ä—É—é PDF‚Ä¶")
        pdf_bytes = make_pdf_bytes(f"–ê—É–¥–∏—Ç —Å–∞–π—Ç–∞: {domain_disp}", main_items, sections, gpt_blocks)

        USER_REPORTS[message.from_user.id] = {"text": tech_text, "pdf_path": f"/tmp_report_{message.from_user.id}.pdf"}  # –ø—É—Ç—å —É—Å–ª–æ–≤–Ω—ã–π
        USER_REPORTS[message.from_user.id]["pdf_bytes"] = pdf_bytes  # type: ignore

        kb = InlineKeyboardMarkup(inline_keyboard=[[InlineKeyboardButton(text="üì• –°–∫–∞—á–∞—Ç—å PDF", callback_data="pdf")]])
        await message.answer("–ì–æ—Ç–æ–≤–æ ‚úÖ", reply_markup=kb)

    except Exception as e:
        await message.answer(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: <code>{short(str(e), 300)}</code>")

@dp.callback_query(F.data == "pdf")
async def on_pdf(call: CallbackQuery):
    rep = USER_REPORTS.get(call.from_user.id)
    if not rep or "pdf_bytes" not in rep:
        await call.answer("–û—Ç—á—ë—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –û—Ç–ø—Ä–∞–≤—å—Ç–µ URL –µ—â—ë —Ä–∞–∑.", show_alert=True)
        return
    pdf_bytes = rep["pdf_bytes"]  # type: ignore
    filename = "site-audit.pdf"
    await call.message.answer_document(types.BufferedInputFile(pdf_bytes, filename=filename))
    await call.answer()

# ================== run ==================
async def main():
    print("Bot is running‚Ä¶")
    await dp.start_polling(bot)

if __name__ == "__main__":

    os.environ["OPENAI_API_KEY"] = "sk-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"

    asyncio.run(main())
